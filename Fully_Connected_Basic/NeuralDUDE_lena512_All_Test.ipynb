{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"THEANO_FLAGS\"]=\"mode=FAST_RUN, device=gpu3,floatX=float32\"\n",
    "import theano\n",
    "import keras\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import Binary_DUDE as DUDE\n",
    "import Binary_N_DUDE as N_DUDE\n",
    "\n",
    "from numpy import *\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adam, Adadelta\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Pre-Generated Data Load ###\n",
    "data=np.load('/HDD/user/yoon/Yoon_SV4/N-DUDE_SV4/NeuralDUDE_Delta_Variation/Data_Generation&Save/Neural_dude_Data_lena512.npz')\n",
    "\n",
    "nb_classes=data['nb_classes']\n",
    "delta=data['delta']\n",
    "dp=data['dp']\n",
    "loss_lines=data['loss_lines']\n",
    "imarray=data['imarray']\n",
    "im_bin=data['im_bin']\n",
    "x=data['x']\n",
    "z=data['z']\n",
    "z_two=data['z_two']\n",
    "L=data['L']\n",
    "L_lower=data['L_lower']\n",
    "L_new=data['L_new']\n",
    "offset=data['offset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Make data which this experiment needs ###\n",
    "n=imarray.shape[0]*imarray.shape[1]\n",
    "alpha_size=2\n",
    "mapping_size=3\n",
    "\n",
    "Z=[]\n",
    "P=np.zeros((len(delta),imarray.shape[0],imarray.shape[1],nb_classes),dtype=np.int)\n",
    "P_padding=np.zeros((len(delta), imarray.shape[0]+2*loss_lines, imarray.shape[1]+2*loss_lines, nb_classes), \n",
    "                   dtype=np.int)\n",
    "\n",
    "for i in range(len(delta)):\n",
    "    Temp=np_utils.to_categorical(z[i],nb_classes) ## [0]->[1 0], [1]->[0 1]\n",
    "    Temp=Temp.reshape(nb_classes*n,)\n",
    "    Z=np.append(Z,Temp)\n",
    "    \n",
    "Z=np.reshape(Z, (len(delta)*n,nb_classes))\n",
    "\n",
    "for i in range(len(delta)):\n",
    "    P[i]=np.reshape(Z[i*n:(i+1)*n,],(imarray.shape[0], imarray.shape[1], nb_classes))\n",
    "\n",
    "for i in range(len(delta)):\n",
    "    Temp=hstack((zeros((imarray.shape[0],loss_lines,nb_classes)), P[i], zeros((imarray.shape[0],loss_lines,nb_classes))))\n",
    "    P_padding[i]=vstack((zeros((loss_lines,Temp.shape[1],nb_classes)), Temp, zeros((loss_lines,Temp.shape[1],nb_classes))))\n",
    "\n",
    "### True Loss Mat ###\n",
    "Error_One_DUDE=zeros(len(delta)*k_max)\n",
    "Error_Two_DUDE=Error_One_DUDE.copy()\n",
    "\n",
    "Error_One_NN_DUDE=Error_One_DUDE.copy()\n",
    "Error_Two_NN_DUDE=Error_One_DUDE.copy()\n",
    "Error_Two_NN_DUDE_PD=Error_One_DUDE.copy()\n",
    "\n",
    "Error_One_NN_DUDE_LB=Error_One_DUDE.copy()\n",
    "Error_Two_NN_DUDE_LB=Error_One_DUDE.copy()\n",
    "Error_Two_NN_DUDE_PD_LB=Error_One_DUDE.copy()\n",
    "\n",
    "### Mat initialization ###\n",
    "for i in range(len(delta)):\n",
    "    Error_One_DUDE[i*k_max]=delta[i]\n",
    "    Error_Two_DUDE[i*k_max]=delta[i]\n",
    "    \n",
    "    Error_One_NN_DUDE[i*k_max]=delta[i]\n",
    "    Error_Two_NN_DUDE[i*k_max]=delta[i]\n",
    "    Error_Two_NN_DUDE_PD[i*k_max]=delta[i]\n",
    "    \n",
    "    Error_One_NN_DUDE_LB[i*k_max]=delta[i]\n",
    "    Error_Two_NN_DUDE_LB[i*k_max]=delta[i]\n",
    "\n",
    "num_dude=2\n",
    "num_nn=5\n",
    "DUDE_Time=zeros((len(delta),(num_dude+1)*k_max)) # for tot_time, +1\n",
    "NN_Time=zeros((len(delta),(num_nn+1)*k_max))\n",
    "\n",
    "### Est Loss Mat ###\n",
    "for i in range(len(delta)):\n",
    "    Est_Loss_NN_DUDE=zeros((3,len(delta)*k_max))\n",
    "    Est_Loss_NN_DUDE[0,i*k_max]=delta[i] # 1-D N-DUDE\n",
    "    Est_Loss_NN_DUDE[1,i*k_max]=delta[i] # 2-D N-DUDE\n",
    "    Est_Loss_NN_DUDE[2,i*k_max]=delta[i] # 2-D N-DUDE Padding\n",
    "    \n",
    "### X_hat Mat ###\n",
    "X_hat_One_DUDE=np.zeros((len(delta)*k_max,n))\n",
    "X_hat_Two_DUDE=X_hat_One_DUDE.copy()\n",
    "\n",
    "X_hat_One_NN_DUDE=X_hat_One_DUDE.copy()\n",
    "X_hat_Two_NN_DUDE=X_hat_One_DUDE.copy()\n",
    "X_hat_Two_NN_DUDE_PD=X_hat_One_DUDE.copy()\n",
    "\n",
    "X_hat_One_NN_DUDE_LB=X_hat_One_DUDE.copy()\n",
    "X_hat_Two_NN_DUDE_LB=X_hat_One_DUDE.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(delta)):\n",
    "    print \"##### delta=%0.2f #####\" % delta[i]\n",
    "    for k in range(1,k_max+1):\n",
    "        print 'k=',k\n",
    "        Total_DUDE_Start=time.time()\n",
    "        One_DUDE_Start=time.time()\n",
    "        \n",
    "        ### 1-D DUDE ###\n",
    "        s_hat=DUDE.One_DUDE(z[i],k,delta[i])\n",
    "        x_dude_hat=DUDE.denoise_with_s(z[i],s_hat,k)\n",
    "        error_dude=DUDE.error_rate(x,x_dude_hat)\n",
    "        print '1-D DUDE=',error_dude\n",
    "        \n",
    "        Error_One_DUDE[k_max*i+k]=error_dude\n",
    "        X_hat_One_DUDE[k_max*i+k,:]=x_dude_hat\n",
    "        \n",
    "        One_DUDE_End=time.time()\n",
    "        One_DUDE_Duration=One_DUDE_End-One_DUDE_Start\n",
    "        \n",
    "        ### 2-D DUDE ###\n",
    "        Two_DUDE_Start=time.time()\n",
    "        s_hat_two=DUDE.Two_DUDE(z_two[i],k,delta[i],n,offset) \n",
    "        x_dude_hat_two=DUDE.denoise_with_s_Two_DUDE(z_two[i],s_hat_two,k)\n",
    "        error_dude_two=DUDE.error_rate(x,x_dude_hat_two)\n",
    "        print '2-D DUDE=',error_dude_two\n",
    "        \n",
    "        Error_Two_DUDE[k_max*i+k]=error_dude_two\n",
    "        X_hat_Two_DUDE[k_max*i+k,:]=x_dude_hat_two\n",
    "        \n",
    "        Two_DUDE_End=time.time()\n",
    "        Two_DUDE_Duration=Two_DUDE_End-Two_DUDE_Start\n",
    "        \n",
    "        Total_DUDE_End=time.time()\n",
    "        Total_DUDE_Duration=Total_DUDE_End-Total_DUDE_Start\n",
    "        \n",
    "        DUDE_Time[i,(num_dude+1)*(k-1):(num_dude+1)*k]=[One_DUDE_Duration, Two_DUDE_Duration, Total_DUDE_Duration]\n",
    "        \n",
    "        ### 1-D N-DUDE ###\n",
    "        Total_NN_Start=time.time()\n",
    "        One_NN_Start=time.time()\n",
    "        C,Y = N_DUDE.make_data_for_One_NN_DUDE(Z[i*n:(i+1)*n],k,L_new[i*alpha_size:(i+1)*alpha_size,],nb_classes,n)\n",
    "        \n",
    "        model=Sequential()\n",
    "        model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(3,init='he_normal'))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        rms=RMSprop(lr=0.001, rho=0.9, epsilon=1e-06,clipnorm=1.5)\n",
    "        adagrad=Adagrad(clipnorm=1.5)\n",
    "        adam=Adam()\n",
    "        adadelta=Adadelta()\n",
    "        sgd=SGD(lr=0.01,decay=1e-6,momentum=0.95, nesterov=True, clipnorm=1.0)\n",
    "        \n",
    "        model.compile(loss='poisson', optimizer=adam)\n",
    "        model.fit(C,Y,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0, validation_data=(C, Y))\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "        \n",
    "        pred_class=model.predict_classes(C, batch_size=200, verbose=0)\n",
    "        s_nn_hat=hstack((zeros(k),pred_class,zeros(k)))\n",
    "        x_nn_hat=N_DUDE.denoise_with_s(z[i],s_nn_hat,k)\n",
    "        error_nn=N_DUDE.error_rate(x,x_nn_hat)\n",
    "        \n",
    "        print '1-D N-DUDE=', error_nn\n",
    "        \n",
    "        Error_One_NN_DUDE[k_max*i+k]=error_nn\n",
    "        X_hat_One_NN_DUDE[k_max*i+k,:]=x_nn_hat\n",
    "        \n",
    "        s_class=3\n",
    "        s_nn_hat_cat=np_utils.to_categorical(s_nn_hat,s_class)\n",
    "        emp_dist=dot(Z[i*n:(i+1)*n,],L[i*alpha_size:(i+1)*alpha_size,])\n",
    "        est_loss_nn_dude=mean(sum(emp_dist*s_nn_hat_cat,axis=1))\n",
    "        Est_Loss_NN_DUDE[0,k_max*i+k]=est_loss_nn_dude\n",
    "        \n",
    "        One_NN_End=time.time()\n",
    "        One_NN_Duration=One_NN_End-One_NN_Start\n",
    "        \n",
    "        ### 2-D N-DUDE ###\n",
    "        Two_NN_Start=time.time()\n",
    "        C_two,Y_two = N_DUDE.make_data_for_Two_NN_DUDE(P[i],Z[i*n:(i+1)*n],k,L_new[i*alpha_size:(i+1)*alpha_size,],\n",
    "                                                       nb_classes,n,offset)\n",
    "        \n",
    "        model=Sequential()\n",
    "        model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(3,init='he_normal'))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='poisson', optimizer=adam)\n",
    "        model.fit(C_two,Y_two,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0, validation_data=(C_two, Y_two))\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "    \n",
    "        pred_class_two=model.predict_classes(C_two, batch_size=200, verbose=0)\n",
    "        s_nn_hat_two=N_DUDE.mapping_mat_resize(pred_class_two,k,n)\n",
    "        x_nn_hat_two=N_DUDE.denoise_with_s_Two_NN_DUDE(z[i],s_nn_hat_two) \n",
    "        error_nn_two=N_DUDE.error_rate(x,x_nn_hat_two)\n",
    "        print '2-D N-DUDE=', error_nn_two\n",
    "        \n",
    "        Error_Two_NN_DUDE[k_max*i+k]=error_nn_two\n",
    "        X_hat_Two_NN_DUDE[k_max*i+k,:]=x_nn_hat_two\n",
    "           \n",
    "        s_class_two=3\n",
    "        s_nn_hat_cat_two=np_utils.to_categorical(s_nn_hat_two,s_class_two)\n",
    "        emp_dist_two=dot(Z[i*n:(i+1)*n,],L[i*alpha_size:(i+1)*alpha_size,])\n",
    "        est_loss_nn_dude_two=mean(sum(emp_dist_two*s_nn_hat_cat_two,axis=1))\n",
    "        Est_Loss_NN_DUDE[1,k_max*i+k]=est_loss_nn_dude_two\n",
    "        \n",
    "        Two_NN_End=time.time()\n",
    "        Two_NN_Duration=Two_NN_End-Two_NN_Start\n",
    "        \n",
    "        ### 2-D N-DUDE Padding ###\n",
    "        Two_NN_PD_Start=time.time()\n",
    "        C_two_padding,Y_two_padding = N_DUDE.make_data_for_Two_NN_DUDE_PD(P_padding[i],P[i],k,\n",
    "                                                                          L_new[i*alpha_size:(i+1)*alpha_size,],\n",
    "                                                                          nb_classes,n,offset,dp)\n",
    "        model=Sequential()\n",
    "        model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(3,init='he_normal'))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='poisson', optimizer=adam)\n",
    "        model.fit(C_two_padding,Y_two_padding,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0, \n",
    "                  validation_data=(C_two_padding, Y_two_padding))\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "    \n",
    "        s_nn_hat_two_padding=model.predict_classes(C_two_padding, batch_size=200, verbose=0)\n",
    "        x_nn_hat_two_padding=N_DUDE.denoise_with_s_Two_NN_DUDE(z[i],s_nn_hat_two_padding) \n",
    "        error_nn_two_padding=N_DUDE.error_rate(x,x_nn_hat_two_padding)\n",
    "        print '2-D N-DUDE_Padding=', error_nn_two_padding\n",
    "        \n",
    "        Error_Two_NN_DUDE_PD[k_max*i+k]=error_nn_two_padding\n",
    "        X_hat_Two_NN_DUDE_PD[k_max*i+k,:]=x_nn_hat_two_padding\n",
    "       \n",
    "        s_class_two_padding=3\n",
    "        s_nn_hat_cat_two_padding=np_utils.to_categorical(s_nn_hat_two_padding,s_class_two_padding)\n",
    "        emp_dist_two_padding=dot(Z[i*n:(i+1)*n,],L[i*alpha_size:(i+1)*alpha_size,])\n",
    "        est_loss_nn_dude_two_padding=mean(sum(emp_dist_two_padding*s_nn_hat_cat_two_padding,axis=1))\n",
    "        Est_Loss_NN_DUDE[2,k_max*i+k]=est_loss_nn_dude_two_padding\n",
    "        \n",
    "        Two_NN_PD_End=time.time()\n",
    "        Two_NN_PD_Duration=Two_NN_PD_End-Two_NN_PD_Start\n",
    "        \n",
    "        ### 1-D N-DUDE Bound ###\n",
    "        One_NN_LB_Start=time.time()\n",
    "        C_lower,Y_lower = N_DUDE.make_data_for_One_NN_DUDE_LB(Z[i*n:(i+1)*n],k,L_lower,x,z[i],nb_classes,n)\n",
    "    \n",
    "        model=Sequential()\n",
    "        model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(3,init='he_normal'))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='poisson', optimizer=adam)\n",
    "        model.fit(C_lower,Y_lower,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0, validation_data=(C_lower, Y_lower))\n",
    "    \n",
    "        # -----------------------------------------------------\n",
    "    \n",
    "        pred_class_lower=model.predict_classes(C_lower, batch_size=200, verbose=0)\n",
    "        s_nn_hat_lower=hstack((zeros(k),pred_class_lower,zeros(k)))\n",
    "        x_nn_hat_lower=N_DUDE.denoise_with_s(z[i],s_nn_hat_lower,k)\n",
    "        error_nn_lower=N_DUDE.error_rate(x,x_nn_hat_lower)\n",
    "    \n",
    "        print '1-D N-DUDE_Bound=', error_nn_lower\n",
    "        Error_One_NN_DUDE_LB[k_max*i+k]=error_nn_lower\n",
    "    \n",
    "        X_hat_One_NN_DUDE_LB[k_max*i+k,:]=x_nn_hat_lower\n",
    "        One_NN_LB_End=time.time()\n",
    "        One_NN_LB_Duration=One_NN_LB_End-One_NN_LB_Start\n",
    "\n",
    "        ### 2-D N-DUDE Bound ###\n",
    "        Two_NN_LB_Start=time.time()\n",
    "        C_two_lower,Y_two_lower = N_DUDE.make_data_for_Two_NN_DUDE_LB(P[i],k,L_lower,im_bin,nb_classes,\n",
    "                                                                      n,offset)\n",
    "\n",
    "        model=Sequential()\n",
    "        model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40,init='he_normal'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(3,init='he_normal'))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='poisson', optimizer=adam)\n",
    "        model.fit(C_two_lower,Y_two_lower,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0, \n",
    "                  validation_data=(C_two_lower, Y_two_lower))\n",
    "    \n",
    "        # -----------------------------------------------------\n",
    "    \n",
    "        pred_class_two_lower=model.predict_classes(C_two_lower, batch_size=200, verbose=0)\n",
    "        s_nn_hat_two_lower=N_DUDE.mapping_mat_resize(pred_class_two_lower,k,n)\n",
    "        x_nn_hat_two_lower=N_DUDE.denoise_with_s_Two_NN_DUDE(z[i],s_nn_hat_two_lower)\n",
    "        error_nn_two_lower=N_DUDE.error_rate(x,x_nn_hat_two_lower)\n",
    "    \n",
    "        print '2-D N-DUDE_Bound=', error_nn_two_lower\n",
    "        Error_Two_NN_DUDE_LB[k_max*i+k]=error_nn_two_lower\n",
    "        X_hat_Two_NN_DUDE_LB[k_max*i+k,:]=x_nn_hat_two_lower\n",
    "        \n",
    "        Two_NN_LB_End=time.time()\n",
    "        Two_NN_LB_Duration=Two_NN_LB_End-Two_NN_LB_Start\n",
    "        \n",
    "        Total_NN_End=time.time()\n",
    "        Total_NN_Duration=Total_NN_End-Total_NN_Start\n",
    "        \n",
    "        NN_Time[i,(num_nn+1)*(k-1):(num_nn+1)*k]=[One_NN_Duration, Two_NN_Duration, Two_NN_PD_Duration, \n",
    "                                                  One_NN_LB_Duration, Two_NN_LB_Duration, Total_NN_Duration]\n",
    "        print \"\"\n",
    "        print \"1-D_DUDE time=\",One_DUDE_Duration, \"2-D_DUDE time=\",Two_DUDE_Duration\n",
    "        print \"Total_DUDE time=\",Total_DUDE_Duration\n",
    "        print '1-D_NN_Time=', One_NN_Duration, '2-D_NN_time=', Two_NN_Duration, '2-D_NN_PD_Time=', Two_NN_PD_Duration \n",
    "        print '1-D_NN_Bound_Time=', One_NN_LB_Duration, '2-D_NN_Bound_Time=', Two_NN_LB_Duration\n",
    "        print 'Total_NN_Time=', Total_NN_Duration\n",
    "        print 'Total_Time=', Total_NN_Duration+Total_DUDE_Duration\n",
    "        print \"---------------------------------------------------\"\n",
    "        \n",
    "        res_file='/home/mind-ws1/Yoon_WS1/N-DUDE_WS1/NeuralDUDE_Delta_Variation/Result_Plot/Neural_DUDE_Result_lena512'\n",
    "        np.savez(res_file,Error_One_DUDE=Error_One_DUDE, Error_Two_DUDE=Error_Two_DUDE, Error_One_NN_DUDE=Error_One_NN_DUDE,\n",
    "                 Error_One_NN_DUDE_LB=Error_One_NN_DUDE_LB, Error_Two_NN_DUDE=Error_Two_NN_DUDE,\n",
    "                 Error_Two_NN_DUDE_LB=Error_Two_NN_DUDE_LB, Error_Two_NN_DUDE_PD=Error_Two_NN_DUDE_PD,\n",
    "                 Est_Loss_NN_DUDE=Est_Loss_NN_DUDE, X_hat_One_DUDE=X_hat_One_DUDE,\n",
    "                 X_hat_Two_DUDE=X_hat_Two_DUDE, X_hat_One_NN_DUDE=X_hat_One_NN_DUDE, X_hat_One_NN_DUDE_LB=X_hat_One_NN_DUDE_LB,\n",
    "                 X_hat_Two_NN_DUDE=X_hat_Two_NN_DUDE, X_hat_Two_NN_DUDE_LB=X_hat_Two_NN_DUDE_LB,\n",
    "                 X_hat_Two_NN_DUDE_PD=X_hat_Two_NN_DUDE_PD, DUDE_Time=DUDE_Time, NN_Time=NN_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Only Delta=0.1 case ###\n",
    "print \"##### delta=%0.2f #####\" % delta[9]\n",
    "for k in range(1,k_max+1):\n",
    "    print 'k=',k\n",
    "    Total_DUDE_Start=time.time()\n",
    "    One_DUDE_Start=time.time()\n",
    "        \n",
    "    ### 1-D DUDE ###\n",
    "    s_hat=DUDE.One_DUDE(z[9],k,delta[9])\n",
    "    x_dude_hat=DUDE.denoise_with_s(z[9],s_hat,k)\n",
    "    error_dude=DUDE.error_rate(x,x_dude_hat)\n",
    "    print '1-D DUDE=',error_dude\n",
    "        \n",
    "    Error_One_DUDE[k]=error_dude\n",
    "    X_hat_One_DUDE[k,:]=x_dude_hat\n",
    "        \n",
    "    One_DUDE_End=time.time()\n",
    "    One_DUDE_Duration=One_DUDE_End-One_DUDE_Start\n",
    "        \n",
    "    ### 2-D DUDE ###\n",
    "    Two_DUDE_Start=time.time()\n",
    "    s_hat_two=DUDE.Two_DUDE(z_two[9],k,delta[9],n,offset)\n",
    "    x_dude_hat_two=DUDE.denoise_with_s_Two_DUDE(z_two[9],s_hat_two,k)\n",
    "    error_dude_two=DUDE.error_rate(x,x_dude_hat_two)\n",
    "    print '2-D DUDE=',error_dude_two\n",
    "        \n",
    "    Error_Two_DUDE[k]=error_dude_two\n",
    "    X_hat_Two_DUDE[k,:]=x_dude_hat_two\n",
    "        \n",
    "    Two_DUDE_End=time.time()\n",
    "    Two_DUDE_Duration=Two_DUDE_End-Two_DUDE_Start\n",
    "        \n",
    "    Total_DUDE_End=time.time()\n",
    "    Total_DUDE_Duration=Total_DUDE_End-Total_DUDE_Start\n",
    "        \n",
    "    DUDE_Time[(num_dude+1)*(k-1):(num_dude+1)*k]=[One_DUDE_Duration, Two_DUDE_Duration, Total_DUDE_Duration]\n",
    "        \n",
    "    ### 1-D N-DUDE ###\n",
    "    Total_NN_Start=time.time()\n",
    "    One_NN_Start=time.time()\n",
    "    C,Y = N_DUDE.make_data_for_One_NN_DUDE(Z,k,L_new[9*alpha_size:(9+1)*alpha_size,],nb_classes,n)\n",
    "        \n",
    "    model=Sequential()\n",
    "    model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(3,init='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    rms=RMSprop(lr=0.001, rho=0.9, epsilon=1e-06,clipnorm=1.5)\n",
    "    adagrad=Adagrad(clipnorm=1.5)\n",
    "    adam=Adam()\n",
    "    adadelta=Adadelta()\n",
    "    sgd=SGD(lr=0.01,decay=1e-6,momentum=0.95, nesterov=True, clipnorm=1.0)\n",
    "        \n",
    "    model.compile(loss='poisson', optimizer=adam)\n",
    "    model.fit(C,Y,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0, validation_data=(C, Y))\n",
    "        \n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    pred_class=model.predict_classes(C, batch_size=200, verbose=0)\n",
    "    s_nn_hat=hstack((zeros(k),pred_class,zeros(k)))\n",
    "    x_nn_hat=N_DUDE.denoise_with_s(z[9],s_nn_hat,k)\n",
    "    error_nn=N_DUDE.error_rate(x,x_nn_hat)\n",
    "        \n",
    "    print '1-D N-DUDE=', error_nn\n",
    "        \n",
    "    Error_One_NN_DUDE[k]=error_nn\n",
    "    X_hat_One_NN_DUDE[k,:]=x_nn_hat\n",
    "        \n",
    "    s_class=3\n",
    "    s_nn_hat_cat=np_utils.to_categorical(s_nn_hat,s_class)\n",
    "    emp_dist=dot(Z,L[9*alpha_size:(9+1)*alpha_size,])\n",
    "    est_loss_nn_dude=mean(sum(emp_dist*s_nn_hat_cat,axis=1))\n",
    "    Est_Loss_NN_DUDE[0,k]=est_loss_nn_dude\n",
    "        \n",
    "    One_NN_End=time.time()\n",
    "    One_NN_Duration=One_NN_End-One_NN_Start\n",
    "        \n",
    "    ### 2-D N-DUDE ###\n",
    "    Two_NN_Start=time.time()\n",
    "    C_two,Y_two = N_DUDE.make_data_for_Two_NN_DUDE(P,Z,k,L_new[9*alpha_size:(9+1)*alpha_size,],\n",
    "                                                   nb_classes,n,offset)\n",
    "        \n",
    "    model=Sequential()\n",
    "    model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(3,init='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='poisson', optimizer=adam)\n",
    "    model.fit(C_two,Y_two,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0, validation_data=(C_two, Y_two))\n",
    "        \n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    pred_class_two=model.predict_classes(C_two, batch_size=200, verbose=0)\n",
    "    s_nn_hat_two=N_DUDE.mapping_mat_resize(pred_class_two,k,n)\n",
    "    x_nn_hat_two=N_DUDE.denoise_with_s_Two_NN_DUDE(z[9],s_nn_hat_two) \n",
    "    error_nn_two=N_DUDE.error_rate(x,x_nn_hat_two)\n",
    "    print '2-D N-DUDE=', error_nn_two\n",
    "        \n",
    "    Error_Two_NN_DUDE[k]=error_nn_two\n",
    "    X_hat_Two_NN_DUDE[k,:]=x_nn_hat_two\n",
    "           \n",
    "    s_class_two=3\n",
    "    s_nn_hat_cat_two=np_utils.to_categorical(s_nn_hat_two,s_class_two)\n",
    "    emp_dist_two=dot(Z,L[9*alpha_size:(9+1)*alpha_size,])\n",
    "    est_loss_nn_dude_two=mean(sum(emp_dist_two*s_nn_hat_cat_two,axis=1))\n",
    "    Est_Loss_NN_DUDE[1,k]=est_loss_nn_dude_two\n",
    "        \n",
    "    Two_NN_End=time.time()\n",
    "    Two_NN_Duration=Two_NN_End-Two_NN_Start\n",
    "        \n",
    "    ### 2-D N-DUDE Padding ###\n",
    "    Two_NN_PD_Start=time.time()\n",
    "    C_two_padding,Y_two_padding = N_DUDE.make_data_for_Two_NN_DUDE_PD(P_padding,P,k,\n",
    "                                                                      L_new[9*alpha_size:(9+1)*alpha_size,],\n",
    "                                                                      nb_classes,n,offset,dp)\n",
    "    model=Sequential()\n",
    "    model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(3,init='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='poisson', optimizer=adam)\n",
    "    model.fit(C_two_padding,Y_two_padding,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0, \n",
    "              validation_data=(C_two_padding, Y_two_padding))\n",
    "        \n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    s_nn_hat_two_padding=model.predict_classes(C_two_padding, batch_size=200, verbose=0)\n",
    "    x_nn_hat_two_padding=N_DUDE.denoise_with_s_Two_NN_DUDE(z[9],s_nn_hat_two_padding) \n",
    "    error_nn_two_padding=N_DUDE.error_rate(x,x_nn_hat_two_padding)\n",
    "    print '2-D N-DUDE_Padding=', error_nn_two_padding\n",
    "        \n",
    "    Error_Two_NN_DUDE_PD[k]=error_nn_two_padding\n",
    "    X_hat_Two_NN_DUDE_PD[k,:]=x_nn_hat_two_padding\n",
    "       \n",
    "    s_class_two_padding=3\n",
    "    s_nn_hat_cat_two_padding=np_utils.to_categorical(s_nn_hat_two_padding,s_class_two_padding)\n",
    "    emp_dist_two_padding=dot(Z,L[9*alpha_size:(9+1)*alpha_size,])\n",
    "    est_loss_nn_dude_two_padding=mean(sum(emp_dist_two_padding*s_nn_hat_cat_two_padding,axis=1))\n",
    "    Est_Loss_NN_DUDE[2,k]=est_loss_nn_dude_two_padding\n",
    "        \n",
    "    Two_NN_PD_End=time.time()\n",
    "    Two_NN_PD_Duration=Two_NN_PD_End-Two_NN_PD_Start\n",
    "        \n",
    "    ### 1-D N-DUDE Bound ###\n",
    "    One_NN_LB_Start=time.time()\n",
    "    C_lower,Y_lower = N_DUDE.make_data_for_One_NN_DUDE_LB(Z,k,L_lower,x,z[9],nb_classes,n)\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(3,init='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='poisson', optimizer=adam)\n",
    "    model.fit(C_lower,Y_lower,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0, validation_data=(C_lower, Y_lower))\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    pred_class_lower=model.predict_classes(C_lower, batch_size=200, verbose=0)\n",
    "    s_nn_hat_lower=hstack((zeros(k),pred_class_lower,zeros(k)))\n",
    "    x_nn_hat_lower=N_DUDE.denoise_with_s(z[9],s_nn_hat_lower,k)\n",
    "    error_nn_lower=N_DUDE.error_rate(x,x_nn_hat_lower)\n",
    "    \n",
    "    print '1-D N-DUDE_Bound=', error_nn_lower\n",
    "    Error_One_NN_DUDE_LB[k]=error_nn_lower\n",
    "    \n",
    "    X_hat_One_NN_DUDE_LB[k,:]=x_nn_hat_lower\n",
    "    One_NN_LB_End=time.time()\n",
    "    One_NN_LB_Duration=One_NN_LB_End-One_NN_LB_Start\n",
    "\n",
    "    ### 2-D N-DUDE Bound ###\n",
    "    Two_NN_LB_Start=time.time()\n",
    "    C_two_lower,Y_two_lower = N_DUDE.make_data_for_Two_NN_DUDE_LB(P,k,L_lower,im_bin,nb_classes,\n",
    "                                                                      n,offset)\n",
    "\n",
    "    model=Sequential()\n",
    "    model.add(Dense(40,input_dim=2*k*nb_classes,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(40,init='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(3,init='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='poisson', optimizer=adam)\n",
    "    model.fit(C_two_lower,Y_two_lower,nb_epoch=10,batch_size=100,show_accuracy=True, verbose=0,\n",
    "              validation_data=(C_two_lower, Y_two_lower))\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    pred_class_two_lower=model.predict_classes(C_two_lower, batch_size=200, verbose=0)\n",
    "    s_nn_hat_two_lower=N_DUDE.mapping_mat_resize(pred_class_two_lower,k,n)\n",
    "    x_nn_hat_two_lower=N_DUDE.denoise_with_s_Two_NN_DUDE(z[9],s_nn_hat_two_lower)\n",
    "    error_nn_two_lower=N_DUDE.error_rate(x,x_nn_hat_two_lower)\n",
    "    \n",
    "    print '2-D N-DUDE_Bound=', error_nn_two_lower\n",
    "    Error_Two_NN_DUDE_LB[k]=error_nn_two_lower\n",
    "    X_hat_Two_NN_DUDE_LB[k,:]=x_nn_hat_two_lower\n",
    "        \n",
    "    Two_NN_LB_End=time.time()\n",
    "    Two_NN_LB_Duration=Two_NN_LB_End-Two_NN_LB_Start\n",
    "        \n",
    "    Total_NN_End=time.time()\n",
    "    Total_NN_Duration=Total_NN_End-Total_NN_Start\n",
    "        \n",
    "    NN_Time[(num_nn+1)*(k-1):(num_nn+1)*k]=[One_NN_Duration, Two_NN_Duration, Two_NN_PD_Duration,\n",
    "                                              One_NN_LB_Duration, Two_NN_LB_Duration, Total_NN_Duration]\n",
    "       \n",
    "    print \"\"\n",
    "    print \"1-D_DUDE time=\",One_DUDE_Duration, \"2-D_DUDE time=\",Two_DUDE_Duration\n",
    "    print \"Total_DUDE time=\",Total_DUDE_Duration\n",
    "    print '1-D_NN_Time=', One_NN_Duration, '2-D_NN_time=', Two_NN_Duration, '2-D_NN_PD_Time=', Two_NN_PD_Duration\n",
    "    print '1-D_NN_Bound_Time=', One_NN_LB_Duration, '2-D_NN_Bound_Time=', Two_NN_LB_Duration\n",
    "    print 'Total_NN_Time=', Total_NN_Duration\n",
    "    print 'Total_Time=', Total_NN_Duration+Total_DUDE_Duration\n",
    "    print \"---------------------------------------------------\"\n",
    "        \n",
    "    res_file='/home/mind-ws1/Yoon_WS1/N-DUDE_WS1/NeuralDUDE_Delta_Variation/Result_Plot/Neural_DUDE_Result_lena512_One_Delta'\n",
    "    np.savez(res_file,Error_One_DUDE=Error_One_DUDE, Error_Two_DUDE=Error_Two_DUDE, Error_One_NN_DUDE=Error_One_NN_DUDE,\n",
    "             Error_One_NN_DUDE_LB=Error_One_NN_DUDE_LB, Error_Two_NN_DUDE=Error_Two_NN_DUDE,\n",
    "             Error_Two_NN_DUDE_LB=Error_Two_NN_DUDE_LB, Error_Two_NN_DUDE_PD=Error_Two_NN_DUDE_PD,\n",
    "             Est_Loss_NN_DUDE=Est_Loss_NN_DUDE, X_hat_One_DUDE=X_hat_One_DUDE,\n",
    "             X_hat_Two_DUDE=X_hat_Two_DUDE, X_hat_One_NN_DUDE=X_hat_One_NN_DUDE, X_hat_One_NN_DUDE_LB=X_hat_One_NN_DUDE_LB,\n",
    "             X_hat_Two_NN_DUDE=X_hat_Two_NN_DUDE, X_hat_Two_NN_DUDE_LB=X_hat_Two_NN_DUDE_LB,\n",
    "             X_hat_Two_NN_DUDE_PD=X_hat_Two_NN_DUDE_PD, DUDE_Time=DUDE_Time, NN_Time=NN_Time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
